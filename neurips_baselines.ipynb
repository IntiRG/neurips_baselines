{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49665798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gluonts.dataset import Dataset\n",
    "from gluonts.dataset.repository.datasets import (\n",
    "    get_dataset,\n",
    "    dataset_names as gluonts_datasets,\n",
    ")\n",
    "from gluonts.time_feature.seasonality import get_seasonality\n",
    "from utilsforecast.evaluation import evaluate\n",
    "from utilsforecast.losses import mase, smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ff2d6",
   "metadata": {},
   "source": [
    "## ExperimentHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f14cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentHandler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        quantiles: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        results_dir: str = \"./results\",\n",
    "        models_dir: str = \"./models\",\n",
    "    ):\n",
    "        try:\n",
    "            from gluonts.dataset import Dataset\n",
    "            from gluonts.dataset.repository.datasets import (\n",
    "                get_dataset,\n",
    "                dataset_names as gluonts_datasets,\n",
    "            )\n",
    "            from gluonts.time_feature.seasonality import get_seasonality\n",
    "        except ImportError:\n",
    "            print('For ExperimentHandler install via `pip install -e \".[dev, experiment]\"`')\n",
    "\n",
    "        if dataset not in gluonts_datasets:\n",
    "            raise Exception(\n",
    "                f\"dataset {dataset} not found in gluonts \"\n",
    "                f\"available datasets: {', '.join(gluonts_datasets)}\"\n",
    "            )\n",
    "        self.dataset = dataset\n",
    "        self.quantiles = quantiles\n",
    "        self.results_dir = results_dir\n",
    "        self.models_dir = models_dir\n",
    "        # defining datasets\n",
    "        gluonts_dataset = get_dataset(self.dataset)\n",
    "        self.horizon = gluonts_dataset.metadata.prediction_length\n",
    "        if self.horizon is None:\n",
    "            raise Exception(\n",
    "                f\"horizon not found for dataset {self.dataset} \"\n",
    "                \"experiment cannot be run\"\n",
    "            )\n",
    "        self.freq = gluonts_dataset.metadata.freq\n",
    "        self.seasonality = get_seasonality(self.freq)\n",
    "        self.gluonts_train_dataset = gluonts_dataset.train\n",
    "        self.gluonts_test_dataset = gluonts_dataset.test\n",
    "        self._create_dir_if_not_exists(self.results_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_dir_if_not_exists(directory: str):\n",
    "        Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_gluonts_instance_to_df(\n",
    "        ts: dict,\n",
    "        last_n: int | None = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        start_period = ts[\"start\"]\n",
    "        start_ds, freq = start_period.to_timestamp(), start_period.freq\n",
    "        target = ts[\"target\"]\n",
    "        ds = pd.date_range(start=start_ds, freq=freq, periods=len(target))\n",
    "        if last_n is not None:\n",
    "            target = target[-last_n:]\n",
    "            ds = ds[-last_n:]\n",
    "        ts_df = pd.DataFrame({\"unique_id\": ts[\"item_id\"], \"ds\": ds, \"y\": target})\n",
    "        return ts_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform_gluonts_dataset_to_df(\n",
    "        gluonts_dataset,\n",
    "        last_n: int | None = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                ExperimentHandler._transform_gluonts_instance_to_df(ts, last_n=last_n)\n",
    "                for ts in gluonts_dataset\n",
    "            ]\n",
    "        )\n",
    "        df = df.reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    @property\n",
    "    def train_df(self) -> pd.DataFrame:\n",
    "        train_df = self._transform_gluonts_dataset_to_df(self.gluonts_train_dataset)\n",
    "        return train_df\n",
    "\n",
    "    @property\n",
    "    def test_df(self) -> pd.DataFrame:\n",
    "        test_df = self._transform_gluonts_dataset_to_df(\n",
    "            self.gluonts_test_dataset,\n",
    "            last_n=self.horizon,\n",
    "        )\n",
    "        return test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ece7e85",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e30ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _divide_no_nan(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Auxiliary funtion to handle divide by 0\n",
    "    \"\"\"\n",
    "    div = a / b\n",
    "    div[div != div] = 0.0\n",
    "    div[div == float(\"inf\")] = 0.0\n",
    "    return div\n",
    "\n",
    "def _metric_protections(\n",
    "    y: np.ndarray, y_hat: np.ndarray, weights: Optional[np.ndarray]\n",
    ") -> None:\n",
    "    if not ((weights is None) or (np.sum(weights) > 0)):\n",
    "        raise Exception(\"Sum of `weights` cannot be 0\")\n",
    "    if not ((weights is None) or (weights.shape == y.shape)):\n",
    "        raise Exception(\n",
    "            f\"Wrong weight dimension weights.shape {weights.shape}, y.shape {y.shape}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4700b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mqloss(\n",
    "    y: np.ndarray,\n",
    "    y_hat: np.ndarray,\n",
    "    quantiles: np.ndarray,\n",
    "    weights: Optional[np.ndarray] = None,\n",
    "    axis: Optional[int] = None,\n",
    ") -> Union[float, np.ndarray]:\n",
    "    \"\"\"Multi-Quantile Loss\n",
    "\n",
    "    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n",
    "    MQL calculates the average multi-quantile Loss for\n",
    "    a given set of quantiles, based on the absolute\n",
    "    difference between predicted quantiles and observed values.\n",
    "\n",
    "    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n",
    "\n",
    "    The limit behavior of MQL allows to measure the accuracy\n",
    "    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with\n",
    "    the continuous ranked probability score (CRPS). This can be achieved\n",
    "    through a numerical integration technique, that discretizes the quantiles\n",
    "    and treats the CRPS integral with a left Riemann approximation, averaging over\n",
    "    uniformly distanced quantiles.\n",
    "\n",
    "    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values.<br>\n",
    "    `y_hat`: numpy array, Predicted values.<br>\n",
    "    `quantiles`: numpy array. Quantiles between 0 and 1, to perform evaluation upon size (n_quantiles).<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `mqloss`: numpy array, (single value).\n",
    "\n",
    "    **References:**<br>\n",
    "    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n",
    "    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = np.ones(y.shape)\n",
    "    if np.sum(quantiles > 1) > 0 or np.sum(quantiles < 0) > 0:\n",
    "        raise Exception(\"`quantiles` need to be between 0 and 1\")\n",
    "\n",
    "    _metric_protections(y, y_hat, weights)\n",
    "    n_q = len(quantiles)\n",
    "\n",
    "    y_rep = np.expand_dims(y, axis=-1)\n",
    "    error = y_hat - y_rep\n",
    "    sq = np.maximum(-error, np.zeros_like(error))\n",
    "    s1_q = np.maximum(error, np.zeros_like(error))\n",
    "    mqloss = quantiles * sq + (1 - quantiles) * s1_q\n",
    "\n",
    "    # Match y/weights dimensions and compute weighted average\n",
    "    weights = np.repeat(np.expand_dims(weights, axis=-1), repeats=n_q, axis=-1)\n",
    "    mqloss = np.average(mqloss, weights=weights, axis=axis)\n",
    "\n",
    "    return mqloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_crps(y, y_hat, quantiles):\n",
    "    \"\"\"Scaled Continues Ranked Probability Score\n",
    "\n",
    "    Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n",
    "    to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n",
    "\n",
    "    This metric averages percentual weighted absolute deviations as\n",
    "    defined by the quantile losses.\n",
    "\n",
    "    $$\n",
    "    \\mathrm{sCRPS}(\\hat{F}_{\\\\tau}, \\mathbf{y}_{\\\\tau}) = \\\\frac{2}{N} \\sum_{i}\n",
    "    \\int^{1}_{0}\n",
    "    \\\\frac{\\mathrm{QL}(\\hat{F}_{i,\\\\tau}, y_{i,\\\\tau})_{q}}{\\sum_{i} | y_{i,\\\\tau} |} dq\n",
    "    $$\n",
    "\n",
    "    where $\\hat{F}_{\\\\tau}$ is the an estimated multivariate distribution, and $y_{i,\\\\tau}$\n",
    "    are its realizations.\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values of size (`n_series`, `horizon`).<br>\n",
    "    `y_hat`: numpy array, Predicted quantiles of size (`n_series`, `horizon`, `n_quantiles`).<br>\n",
    "    `quantiles`: numpy array,(`n_quantiles`). Quantiles to estimate from the distribution of y.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `loss`: float.\n",
    "\n",
    "    **References:**<br>\n",
    "    - [Gneiting, Tilmann. (2011). \\\"Quantiles as optimal point forecasts\\\".\n",
    "    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n",
    "    - [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022).\n",
    "    \\\"The M5 uncertainty competition: Results, findings and conclusions\\\".\n",
    "    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n",
    "    - [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021).\n",
    "    \\\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\\\".\n",
    "    Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    norm = np.sum(np.abs(y))\n",
    "    loss = mqloss(y=y, y_hat=y_hat, quantiles=quantiles)\n",
    "    loss = 2 * loss * np.sum(np.ones(y.shape)) / (norm + eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mase(y, y_hat, y_tilde):\n",
    "    \"\"\" Mean Absolute Scaled Error\n",
    "\n",
    "    Calculates relative mean absolute errors, as proposed by Hyndman (2006),\n",
    "    to measure the accuracy of predicted point forecast `y_hat` compared to the observation `y`.\n",
    "    relative to the baseline model y_tilde.\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `loss`: float.\n",
    "\n",
    "    **References:**<br>\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    diff1 = np.sum(np.abs(y-y_hat))\n",
    "    diff2 = np.sum(np.abs(y-y_tilde))\n",
    "    loss = diff1 / (diff2 + eps)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(\n",
    "    y: np.ndarray,\n",
    "    y_hat: np.ndarray,\n",
    "    weights: Optional[np.ndarray] = None,\n",
    "    axis: Optional[int] = None,\n",
    ") -> Union[float, np.ndarray]:\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\n",
    "\n",
    "    Calculates Symmetric Mean Absolute Percentage Error between\n",
    "    `y` and `y_hat`. SMAPE measures the relative prediction\n",
    "    accuracy of a forecasting method by calculating the relative deviation\n",
    "    of the prediction and the observed value scaled by the sum of the\n",
    "    absolute values for the prediction and observed value at a\n",
    "    given time, then averages these devations over the length\n",
    "    of the series. This allows the SMAPE to have bounds between\n",
    "    0% and 200% which is desirable compared to normal MAPE that\n",
    "    may be undetermined when the target is zero.\n",
    "\n",
    "    $$ \\mathrm{sMAPE}_{2}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\\\frac{|y_{\\\\tau}-\\hat{y}_{\\\\tau}|}{|y_{\\\\tau}|+|\\hat{y}_{\\\\tau}|} $$\n",
    "\n",
    "    **Parameters:**<br>\n",
    "    `y`: numpy array, Actual values.<br>\n",
    "    `y_hat`: numpy array, Predicted values.<br>\n",
    "    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n",
    "\n",
    "    **Returns:**<br>\n",
    "    `smape`: numpy array, (single value).\n",
    "\n",
    "    **References:**<br>\n",
    "    [Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)\n",
    "    \"\"\"\n",
    "    _metric_protections(y, y_hat, weights)\n",
    "\n",
    "    delta_y = np.abs(y - y_hat)\n",
    "    scale = np.abs(y) + np.abs(y_hat)\n",
    "    smape = _divide_no_nan(delta_y, scale)\n",
    "    smape = 2 * np.average(smape, weights=weights, axis=axis)\n",
    "\n",
    "    if isinstance(smape, float):\n",
    "        assert smape <= 2, \"SMAPE should be lower than 200\"\n",
    "    else:\n",
    "        assert all(smape <= 2), \"SMAPE should be lower than 200\"\n",
    "\n",
    "    return smape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3519563",
   "metadata": {},
   "source": [
    "## StatsForecast baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632065c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from statsforecast import StatsForecast\n",
    "from statsforecast.models import (\n",
    "    AutoARIMA,\n",
    "    AutoETS,\n",
    "    AutoCES,\n",
    "    DynamicOptimizedTheta,\n",
    "    SeasonalNaive,\n",
    "    ZeroModel,\n",
    ")\n",
    "\n",
    "def ensemble_forecasts(\n",
    "    fcsts_df: pd.DataFrame,\n",
    "    quantiles: List[float],\n",
    "    model_names: List[str],\n",
    "    model_name: str,\n",
    ") -> pd.DataFrame:\n",
    "    fcsts_df[model_name] = fcsts_df[model_names].mean(axis=1).values  # type: ignore\n",
    "    # compute quantiles based on the mean of the forecasts\n",
    "    sigma_models = []\n",
    "    for model in model_names:\n",
    "        fcsts_df[f\"sigma_{model}\"] = fcsts_df[f\"{model}-hi-68.27\"] - fcsts_df[model]\n",
    "        sigma_models.append(f\"sigma_{model}\")\n",
    "    fcsts_df[f\"std_{model_name}\"] = (\n",
    "        fcsts_df[sigma_models].pow(2).sum(axis=1).div(len(sigma_models) ** 2).pow(0.5)\n",
    "    )\n",
    "    z = norm.ppf(quantiles)\n",
    "    q_cols = []\n",
    "    for q, zq in zip(quantiles, z):\n",
    "        q_col = f\"{model_name}-q-{q}\"\n",
    "        fcsts_df[q_col] = fcsts_df[model_name] + zq * fcsts_df[f\"std_{model_name}\"]\n",
    "        q_cols.append(q_col)\n",
    "    fcsts_df = fcsts_df[[\"unique_id\", \"ds\"] + [model_name] + q_cols + [\"SeasonalNaive\"]]\n",
    "    return fcsts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"m1_yearly\"\n",
    "\n",
    "exp = ExperimentHandler(dataset)\n",
    "train_df = exp.train_df\n",
    "test_df = exp.test_df\n",
    "seasonality = exp.seasonality\n",
    "freq = exp.freq\n",
    "horizon = exp.horizon\n",
    "quantiles = exp.quantiles\n",
    "\n",
    "print(quantiles)\n",
    "\n",
    "# def run_statistical_baselines(dataset):\n",
    "# ---------------------------- StatsForecast Code ----------------------------\n",
    "series_per_core = 15\n",
    "n_series = train_df[\"unique_id\"].nunique()\n",
    "n_jobs = min(n_series // series_per_core, os.cpu_count())\n",
    "\n",
    "models = [\n",
    "    AutoARIMA(season_length=seasonality),\n",
    "    AutoETS(season_length=seasonality),\n",
    "    AutoCES(season_length=seasonality),\n",
    "    DynamicOptimizedTheta(season_length=seasonality),\n",
    "    SeasonalNaive(season_length=seasonality)\n",
    "]\n",
    "sf = StatsForecast(\n",
    "    models=models,\n",
    "    fallback_model=ZeroModel(),\n",
    "    freq=freq,\n",
    "    n_jobs=n_jobs,\n",
    ")\n",
    "fcsts_df = sf.forecast(df=train_df, h=horizon, level=[68.27])\n",
    "\n",
    "# ----------------------- Wrangle StatsForecast preds ------------------------\n",
    "models = {'SCUM': ['CES', 'DynamicOptimizedTheta', 'AutoARIMA', 'AutoETS'],\n",
    "          'CES': ['CES'],\n",
    "          'DynamicOptimizedTheta': ['DynamicOptimizedTheta'],\n",
    "          'AutoARIMA': ['AutoARIMA'],\n",
    "          'AutoETS': ['AutoETS']}\n",
    "\n",
    "for model in models:\n",
    "    q_cols = [f\"{model}-q-{q}\" for q in quantiles]\n",
    "    median_col = f\"{model}-q-0.5\"\n",
    "\n",
    "    # Get Quantile Forecasts\n",
    "    preds_df = ensemble_forecasts(\n",
    "        fcsts_df=fcsts_df,\n",
    "        quantiles=quantiles,\n",
    "        model_names=models[model],\n",
    "        model_name=model)\n",
    "\n",
    "    # Reshape datasets into DeepTSv3 format\n",
    "    y_df = train_df.groupby('unique_id', sort=False, )\n",
    "    target_df = test_df.groupby('unique_id', sort=False, )\n",
    "    preds_df = preds_df.groupby('unique_id', sort=False, )\n",
    "  \n",
    "    Y_hat_df = pd.DataFrame({\n",
    "        \"y\": y_df.apply(lambda g: g[\"y\"].to_numpy().tolist(), include_groups=False),\n",
    "        \"ds\": y_df.apply(lambda g: g[\"ds\"].to_numpy().tolist(), include_groups=False),\n",
    "        \"preds\": preds_df.apply(lambda g: g[q_cols].to_numpy().tolist(), include_groups=False),\n",
    "        \"target\": target_df.apply(lambda g: g[\"y\"].to_numpy().tolist(), include_groups=False),\n",
    "        \"median_preds\": preds_df.apply(lambda g: g[median_col].to_numpy().tolist(), include_groups=False),\n",
    "        \"snaive_preds\": preds_df.apply(lambda g: g[\"SeasonalNaive\"].to_numpy().tolist(), include_groups=False),\n",
    "        }).reset_index()\n",
    "\n",
    "    Y_hat_df.to_parquet(f\"{dataset}-{model}.parquet\")\n",
    "\n",
    "for model in models:\n",
    "    if os.path.exists(f\"{dataset}-{model}.parquet\"):\n",
    "        os.remove(f\"{dataset}-{model}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22839233",
   "metadata": {},
   "source": [
    "## StatsForecast Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960dd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "datasets = [\n",
    "    \"m1_monthly\",\n",
    "    \"m1_quarterly\",\n",
    "    \"m1_yearly\",\n",
    "    \"m3_other\",\n",
    "    \"m3_monthly\",\n",
    "    \"m3_quarterly\",\n",
    "    \"m3_yearly\",\n",
    "    \"m4_hourly\",\n",
    "    \"m4_daily\",\n",
    "    'm4_weekly',\n",
    "    \"m4_monthly\",\n",
    "    \"m4_quarterly\",\n",
    "    \"m4_yearly\",\n",
    "    \"tourism_monthly\",\n",
    "    \"tourism_quarterly\",\n",
    "    \"tourism_yearly\",\n",
    "]\n",
    "\n",
    "models = {'SCUM': ['CES', 'DynamicOptimizedTheta', 'AutoARIMA', 'AutoETS'],\n",
    "          'CES': ['CES'],\n",
    "          'DynamicOptimizedTheta': ['DynamicOptimizedTheta'],\n",
    "          'AutoARIMA': ['AutoARIMA'],\n",
    "          'AutoETS': ['AutoETS']}\n",
    "\n",
    "mase_dfs = []\n",
    "scrps_dfs = []\n",
    "smape_dfs = []\n",
    "for dataset in datasets:\n",
    "    mase_df = {'dataset': dataset.split(\"_\", 1)[0], 'freq': dataset.split(\"_\", 1)[1]}\n",
    "    scrps_df = {'dataset': dataset.split(\"_\", 1)[0], 'freq': dataset.split(\"_\", 1)[1]}\n",
    "    smape_df = {'dataset': dataset.split(\"_\", 1)[0], 'freq': dataset.split(\"_\", 1)[1]}\n",
    "    for model in models:\n",
    "        Y_hat_df = pd.read_parquet(f\"neurips_results/{dataset}-{model}.parquet\")\n",
    "\n",
    "        target = np.vstack(Y_hat_df['target'])\n",
    "        preds = np.vstack(Y_hat_df['preds']).flatten()\n",
    "        preds = np.concatenate(preds).reshape(target.shape[0], target.shape[1], -1)\n",
    "        _scrps = scaled_crps(y=target, y_hat=preds, quantiles=np.array(quantiles))\n",
    "\n",
    "        median_preds = np.vstack(Y_hat_df['median_preds']).flatten()\n",
    "        median_preds = median_preds.reshape(target.shape[0], target.shape[1])\n",
    "        snaive_preds = np.vstack(Y_hat_df['snaive_preds']).flatten()\n",
    "        snaive_preds = snaive_preds.reshape(target.shape[0], target.shape[1])\n",
    "        _mase = mase(y=target, y_hat=median_preds, y_tilde=snaive_preds)\n",
    "        _smape = smape(y=target, y_hat=median_preds)\n",
    "\n",
    "        mase_df[model] = _mase\n",
    "        scrps_df[model] = _scrps\n",
    "        smape_df[model] = _smape\n",
    "\n",
    "    mase_dfs.append(mase_df)\n",
    "    scrps_dfs.append(scrps_df)\n",
    "    smape_dfs.append(smape_df)\n",
    "\n",
    "mase_df = pd.DataFrame(mase_dfs)\n",
    "scrps_df = pd.DataFrame(scrps_dfs)\n",
    "smape_df = pd.DataFrame(smape_dfs)\n",
    "\n",
    "Y_hat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16349b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrps_df[\"NBEATS\"]   = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "scrps_df[\"NHITS\"]    = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "scrps_df[\"MQCNN\"]    = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "scrps_df[\"PatchTST\"] = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "scrps_df[\"TFT\"]      = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "scrps_df[\"Chronos\"]  = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "columns = [\"dataset\", \"freq\", \"NBEATS\", \"NHITS\", \"MQCNN\", \"PatchTST\", \"TFT\", \"Chronos\", \"SCUM\", \"CES\", \"DynamicOptimizedTheta\", \"AutoARIMA\", \"AutoETS\"]\n",
    "print(scrps_df[columns].to_latex(index=False,\n",
    "                        formatters={\"name\": str.upper},\n",
    "                        float_format=\"{:.4f}\".format,\n",
    "))\n",
    "print(\"sCRPS\")\n",
    "scrps_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae26a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mase_df[\"NBEATS\"]   = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "mase_df[\"NHITS\"]    = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "mase_df[\"MQCNN\"]    = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "mase_df[\"PatchTST\"] = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "mase_df[\"TFT\"]      = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "mase_df[\"Chronos\"]  = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "columns = [\"dataset\", \"freq\", \"NBEATS\", \"NHITS\", \"MQCNN\", \"PatchTST\", \"TFT\", \"Chronos\", \"SCUM\", \"CES\", \"DynamicOptimizedTheta\", \"AutoARIMA\", \"AutoETS\"]\n",
    "print(mase_df[columns].to_latex(index=False,\n",
    "                       formatters={\"name\": str.upper},\n",
    "                       float_format=\"{:.4f}\".format,\n",
    "))\n",
    "print(\"MASE\")\n",
    "mase_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ac3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_df[\"NBEATS\"]   = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "smape_df[\"NHITS\"]    = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "smape_df[\"MQCNN\"]    = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "smape_df[\"PatchTST\"] = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "smape_df[\"TFT\"]      = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "smape_df[\"Chronos\"]  = [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan]\n",
    "columns = [\"dataset\", \"freq\", \"NBEATS\", \"NHITS\", \"MQCNN\", \"PatchTST\", \"TFT\", \"Chronos\", \"SCUM\", \"CES\", \"DynamicOptimizedTheta\", \"AutoARIMA\", \"AutoETS\"]\n",
    "print(smape_df[columns].to_latex(index=False,\n",
    "                       formatters={\"name\": str.upper},\n",
    "                       float_format=\"{:.4f}\".format,\n",
    "))\n",
    "print(\"sMAPE\")\n",
    "smape_df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd761986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
